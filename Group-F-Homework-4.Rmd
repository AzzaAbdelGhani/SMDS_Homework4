---
title: "Homework 3"
author: "Group B: Abdalghani, Demirbilek, Plasencia Palacios, Spagnolo"
date: "Spring 2020"
output:
  html_document:
    toc: no
header-includes:
- \usepackage{color}
- \definecolor{Purple}{HTML}{911146}
- \definecolor{Orange}{HTML}{CF4A30}
- \setbeamercolor{alerted text}{fg=Orange}
- \setbeamercolor{frametitle}{bg=Purple}
institute: University of Udine & University of Trieste
graphics: yes
fontsize: 10pt
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center', warning=FALSE, message=FALSE, fig.asp=0.625, dev='png', global.par = TRUE, dev.args=list(pointsize=10), fig.path = 'figs/', fig.height = 10, fig.width = 10)
```

```{r setup, include=FALSE}
library(MASS)
library(knitr)
local({
  hook_plot = knit_hooks$get('plot')
  knit_hooks$set(plot = function(x, options) {
    paste0('\n\n----\n\n', hook_plot(x, options))
  })
})
```

# {.tabset}

## Chapter 6 Exercises {.tabset}


### Exercise 8

**Apply the $lm.ridge()$ function to the $litters$ data, using the generalized cross-validation (GCV) criterion to choose the tuning parameter. (GCV is an approximation to cross-validation.)**

**(a) In particular, estimate the coefficients of the model relating $brainwt$ to $bodywt$ and $lsize$ and compare with the results obtained using $lm()$.**


*solution :* 

Firts, we choose the ridge parameter k (or `lambda`) by using Cross-validation :  
```{r basic 1, echo=TRUE}
library(MASS)
library(DAAG)
#select lambda in terms of GCV error
select(lm.ridge(brainwt~. ,data = litters, lambda=seq(0.001, 1, 0.001)))
```

Fit model with the best lambda (=0.118) : 
```{r echo=TRUE}

#fit the Ridge Regression model: 
brainwt_ridge <- lm.ridge(brainwt~., data=litters, lambda=0.118)
brainwt_ridge

#fit the linear model
brainwt_lm <- summary(lm(brainwt~., data=litters))$coefficients[,1]
brainwt_lm
```

We can see that in case of the ridge regression both `bodywt` and `lsize` coefficients are penalized in favor of the intercept, due to the fact that Ridge regression is a penalized likelihood framework. 


**(b) Using both ridge and ordinary regression, estimate the mean brain weight when litter size is 10 and body weight is 7. Use the bootstrap, with case-resampling, to compute approximate 95% percentile confidence intervals using each method. Compare with the interval obtained using $predict.lm()$.**

*solution :* 

The estimated equation is : 

$$ brain.weight = 0.203442601 + (0.005661579 * lsize) + (0.02205028 * bodywt) $$
where : $lsize$ = 10, and $bodywt$ = 7. 


```{r echo=TRUE}
library(boot)
library(lmridge)
# Ridge regression estimate :
ridge.estimate <- coef(brainwt_ridge)[1] + coef(brainwt_ridge)[2]*10 + coef(brainwt_ridge)[3]*7
print(paste("Ridge regerssion estimate : " , ridge.estimate))
 
# Ordinary regression estimate :
new_data <- data.frame(lsize=10, bodywt=7)
predict.lm(lm(brainwt~., data=litters), new_data, interval = "confidence")


# Sampling bootstrap of linear model
lm_func <- function(x, indices) {
  d <- x[indices,] 
  lm_fit <- lm(formula=brainwt~., data=d)
  return(predict(lm_fit, new_data))
} 

# Sampling bootstrap of ridge regression
ridge_func <- function(x, indices) {
  d <- x[indices,]
  ridge_fit <- lmridge(formula=brainwt~., data=d, K = 0.118)
  return(coef(ridge_fit)[1] + coef(ridge_fit)[2]*10 + coef(ridge_fit)[3]*7)
}


# bootstrap-based confidence intervals using boot
lm.boot <- boot(data=litters, statistic=lm_func, R=1000)
ridge.boot <- boot(data=litters, statistic=ridge_func, R=1000)
boot.ci(lm.boot, type="perc")
boot.ci(ridge.boot, type="perc")

```




### Exercise 10


**The data frame `table.b3` in the $MPV$ package contains data on gas mileage and 11 other variables for a sample of 32 automobiles.**

**a) Construct a scatterplot of `y` (mpg) versus `x1` (displacement). Is the relationship between these variables non-linear?**

*solution :* 


```{r echo=TRUE,  message=FALSE, warning=FALSE}
library(MPV)
automobiles <- table.b3

#help(table.b3)

plot(y~x1, data = automobiles, main = "y vs x1 (displacement)",xlab = "x1 (displacement)", ylab = "y", pch = 19,col="purple")

```
The scatterplot shows curvilinear relationship between y and x1.


**b) Use the `xyplot()` function, and `x11` (type of transmission) as a `group` variable. Is a linear model reasonable for these data?**

*solution : *

```{r echo=TRUE,  message=FALSE, warning=FALSE}
library(lattice)
xyplot(y ~ x1 + x2 + x3, data = automobiles, groups = x11, main = "y vs x", pch = 19, key=list(rep=FALSE,columns=1,
                                                                                          text=list(lab=c("manual")),pch=19,points=list(col="cadetblue3"),
                                                                                          text=list(lab=c("automatic")),points=list(col="purple")))

```
Here $x1$ is displacement (cubic in),$x2$ is horsepower and $x3$ is torque. This plot shows that apparent nonlinearity can be explained by the two types of transmission so linear model is reasonable for these data.

**c) Fit the model relating `y` to `x1` and `x11` which gives two lines having possibly different slopes and intercepts. Check the diagnostics. Are there any influential observations? Are there any influential outliers?**

*solution :*

```{r echo=TRUE,  message=FALSE, warning=FALSE}
model0 <- lm(y ~ x1*x11, data = automobiles)
par(mfrow = c(1,4))
plot(model0)

```
Residual plot shows no systematic behaviour but there are some deflection from normal distribution. Observation 5 ,15, 22 influential and 5 is possible outlier according to Cook's distance.


**d) Plot the residuals against the variable `x7` (number of transmission speeds), again using `x11` as a group variable. Is there anything striking about this plot?**

*solution :*

```{r echo=TRUE,  message=FALSE, warning=FALSE}
xyplot(resid(model0) ~ x7, data=automobiles, group=x11, main = "model0_residuals vs x7 (number of transmission speeds)", pch = 19,
       key=list(rep=FALSE,columns=1,text=list(lab=c("manual")),pch=19,points=list(col="cadetblue3"),text=list(lab=c("automatic")),points=list(col="purple")))

```
Here observation 5 is the only car which is manual and has 3-speed transmisson so we can say that our previous foresight was right, 5 is a strong candiate as an outlier.


## Chapter 8 Exercises {.tabset}


### Exercise 2

**In the data set (an artificial one of 3121 patients, that is similar to a subset of the data analyzed in Stiell et al., 2001) $minor.head.injury$, obtain a logistic regression model relating $clinically.important.brain.injury$ to other variables. Patients whose risk is sufficiently high will be sent for CT (computed tomography). Using a risk threshold of 0.025 (2.5%), turn the result into a decision rule for use of CT.**

*solution :*

```{r echo=TRUE}
library(DAAG)

str(head.injury)

injury.fit = glm(formula = clinically.important.brain.injury ~ . , data = head.injury, family = "binomial")
summary(injury.fit) 
#Observe that log(.025/(1-.025)) = -3.66, an increase of 0.84 above the intercept (= -4.50).

# Split the data into train and test sets
n_obs <- nrow(head.injury)
k <- n_obs*0.8
train <- head.injury[1:k,]
test <- head.injury[(k+1):n_obs,] 

#Fit the model 
injury.fit = glm(formula = clinically.important.brain.injury ~ . , data = train, family = "binomial")
#summary(injury.fit)

#get the probabilities of the predictions
probs <- predict(injury.fit, test)

# predict based on the risk threshold 
preds <- ifelse(probs > 0.025, 1, 0)


confusion_mat <- table(actual=test$clinically.important.brain.injury, predicted=preds)
confusion_mat
#print the accuracy 
print(paste('Accuracy', 1-(mean(preds != test$clinically.important.brain.injury))))

```



### Exercise 3

**Consider again the `moths` data set of Section 8.4.**

**a) What happens to the standard error estimates when the `poisson` family is used in `glm()` instead of the `quasipoisson` family?**

*solution : *

```{r echo=TRUE,  message=FALSE, warning=FALSE}
library(DAAG)
#help(moths)
#moths

summary(A.glm <-  glm(A ~ habitat + log(meters), family = quasipoisson, data = moths)) # taken from DAAG book
summary(A.glm <-  glm(A ~ habitat + log(meters), family = poisson, data = moths))

```
We can see that dispersion parameter for quassipoisson is 2.7. One of the explanation to this might be there is a clustering effect with an average cluster size 2.7. Therefore standard errors and p-values from a model that assumed Poisson errors would be misleading because our assumption which is variance is equal to mean doesn't seem fair in this case.. Using quassipoisson has increased the standard errors by a factor of $\sqrt{2.7}=1.64$. So using poisson will reduce the SE by this factor if the poisson family specified.

**b) Analyze the `P` moths, in the same way as the A moths were analyzed. Comment on the effect of transect length.**

*solution : *

```{r echo=TRUE,  message=FALSE, warning=FALSE}
sapply(split(moths$P, moths$habitat), sum)

dotplot(habitat ~ P ,data = moths,  xlab="Number of moths (species P)",
  panel=function(x, y, ...){
    panel.dotplot(x,y, pch=1,type="p", col="red", ...)
    panel.average(x, y, pch=3, cex=1.25, type="p", col="blue")
    },
  key=list(text=list(c("Individual transects", "Mean")),
    points=list(pch=c(1,3), cex=c(1,1.25), col=c("red","blue")),
    columns=2))

# didn't understand why blue is still line even though I specified it as pch=3
#?relevel

moths$habitat <- relevel(moths$habitat, ref = "Lowerside")
summary(P.glm <- glm(P ~ habitat + log(meters), family =  quasipoisson, data = moths)) 

```
The highest numbers are in habitats `SWsoak` and `Disturbed`. Number of moths increases with the transect lenght, by a factor of $e^{0.55} =1.73$ for each one meter increase in transect lenght.